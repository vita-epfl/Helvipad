<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Helvipad: A Real-World Dataset for Omnidirectional Stereo Depth Estimation">
  <meta name="keywords" content="Omnidirectional Imaging, Depth Estimation, Deep Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Helvipad: A Real-World Dataset for Omnidirectional Stereo Depth Estimation</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- DFI-OmniStereo News Banner -->
<div style="background-color: #111; border-bottom: 1px solid #ddd;">
  <div style="max-width: 100%; padding: 10px 0; text-align: center;">
    <span style="color: #fff; font-weight: bold;">
      ğŸš€ Check out our latest model <a href="https://vita-epfl.github.io/DFI-OmniStereo-website/" target="_blank" style="color: #fff; text-decoration: underline;">
        DFI-OmniStereo</a> pushing the state-of-the-art on Helvipad! <em>(08/04/2025)</em>
    </span>
  </div>
</div>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <!-- Title -->
          <h1 class="title is-1 publication-title">
            <span style="font-variant: small-caps;">Helvipad</span>: A Real-World Dataset for Omnidirectional Stereo Depth Estimation
          </h1>
          <h2 class="subtitle has-text-centered"><b>CVPR 2025</b></h2>

          <!-- Authors -->
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://ch.linkedin.com/in/mehdi-zayene-191a64156">Mehdi Zayene</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://people.epfl.ch/jannik.endres">Jannik Endres</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://people.epfl.ch/albias.havolli">Albias Havolli</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://chcorbi.github.io">Charles CorbiÃ¨re</a><sup>1,*</sup>,
            </span><br>
            <span class="author-block">
              <a href="https://people.epfl.ch/salim.cherkaoui">Salim Cherkaoui</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://people.epfl.ch/alexandre.benahmedkontouli">Alexandre Ben Ahmed Kontouli</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://people.epfl.ch/alexandre.alahi">Alexandre Alahi</a><sup>1</sup>
            </span>
          </div>

          <!-- Affiliations -->
          <div class="is-size-5 publication-authors" style="margin-top: 10px;">
            <span class="author-block"><sup>1</sup>EPFL, Switzerland</a>&nbsp</a>&nbsp</a>&nbsp</a>&nbsp</a>&nbsp</a>&nbsp</a>&nbsp</a>&nbsp</a>&nbsp</a>&nbsp</a>&nbsp</a>&nbsp</a>&nbsp  </span>
            <span class="author-block"><sup> 2</sup>TU Darmstadt, Germany</span>
            <span class="project-lead"><small><br><sup>*</sup>project lead</small></span>
          </div>

          <!-- Logo -->
          <div style="margin-top: 20px;">
            <a href="https://www.epfl.ch/labs/vita/">
              <img src="static/images/vita-epfl.png" width="300px" alt="VITA EPFL Logo" />
            </a>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Abstract Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2411.18335"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2411.18335" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/vita-epfl/Helvipad"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/chcorbi/helvipad"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo-pirate.svg"
                           alt="Hugging Face Logo"
                           style="width: 20px; height: 20px; vertical-align: middle;">
                  </span>
                  <span>Dataset</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/helvipad.mov"
                type="video/mp4">
      </video>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Despite considerable progress in stereo depth estimation, omnidirectional imaging remains underexplored,
            mainly due to the lack of appropriate data.
          </p>
          <p>
            We introduce <span style="font-variant: small-caps;">Helvipad</span>,
            a real-world dataset for omnidirectional stereo depth estimation, consisting of 40K frames from video sequences
            across diverse environments, including crowded indoor and outdoor scenes with diverse lighting conditions.
            Collected using two 360Â° cameras in a top-bottom setup and a LiDAR sensor, the dataset includes accurate
            depth and disparity labels by projecting 3D point clouds onto equirectangular images. Additionally, we
            provide an augmented training set with a significantly increased label density by using depth completion.
          </p>
          <p>
            We benchmark leading stereo depth estimation models for both standard and omnidirectional images.
            The results show that while recent stereo methods perform decently, a significant challenge persists in accurately
            estimating depth in omnidirectional imaging. To address this, we introduce necessary adaptations to stereo models,
            achieving improved performance.
          </p>
        </div>
      </div>
    </div>
</section>

<!-- Dataset Statistics Section -->
<section class="section" id="dataset">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Dataset</h2>
    <div class="content has-text-justified">
      <p>
  The <span style="font-variant: small-caps;">Helvipad</span> dataset includes of 39,553 labeled frames from indoor and outdoor scenes under various lighting conditions.

      </p>
      <figure>
        <img src="static/images/front_page.png" alt="Dataset visualisations">
      </figure>
      <p>
      The equipment setup of our data acquisition includes:
    </p>
    <ul>
      <li>
        <strong>2 Ricoh Theta V cameras</strong>, capturing images in 4K/UHD equirectangular format with an initial size of 3840 Ã— 1920 pixels at 30 fps, mounted in a top-bottom arrangement with a 19.1 cm baseline between them.
      </li>
      <li>
        <strong>Ouster OS1-64 LiDAR Sensor</strong>, providing 64 beams, a vertical field of view of 45Â°, and capable of measuring depths from 0 to 120 meters at 10 fps, mounted 45.0 cm below the bottom camera.
      </li>
      <li>
        <strong>Nvidia Jetson Xavier</strong>, serving as the central processor to manage data capture and ensure synchronization across all devices during data collection.
      </li>
    </ul>
      <figure>
        <img src="static/images/lidar_mapping.png" alt="LiDAR to 360Â° Mapping Illustration" style="width: 80%; height: auto;">
      </figure>
      <p>
        Data was extracted from video sequences captured between December 2023 and February 2024. Each sequence is synchronized with its corresponding
        LiDAR point clouds, which are projected on frames to obtain depth maps and disparity maps.
      </p>
      <div class="columns is-centered">
        <div class="column is-one-third">
          <figure>
            <img src="static/images/depth_histograms_all.png" alt="Histogram of Depth Values - All Scenes">
            <figcaption>Depth Distribution - All</figcaption>
          </figure>
        </div>
        <div class="column is-one-third">
          <figure>
            <img src="static/images/depth_histograms_indoor.png" alt="Histogram of Depth Values - Indoor Scenes">
            <figcaption>Depth Distribution - Indoor</figcaption>
          </figure>
        </div>
        <div class="column is-one-third">
          <figure>
            <img src="static/images/depth_histograms_outdoor.png" alt="Histogram of Depth Values - Outdoor Scenes">
            <figcaption>Depth Distribution - Outdoor</figcaption>
          </figure>
        </div>
      </div>
    </div>
    <p>
      Depth values range from 0.5 to 225  meters, with averages of 8.1 meters overall, 5.4 meters for
      indoor scenes, and 9.2 meters for combined day and night outdoor scenes.
    </p>
  </div>
</section>


<!-- Benchmark Results Section -->
<section class="section" id="benchmark-results">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Benchmark Results</h2>
    <div class="content has-text-justified">
      <p>
        We evaluate the performance of multiple state-of-the-art and popular stereo matching methods, both for standard and 360Â° images. All models are trained on a single NVIDIA A100 GPU with
the largest possible batch size to ensure comparable use of computational resources.
      </p>
    <table class="table is-striped is-bordered is-hoverable is-fullwidth">
      <thead>
        <tr>
          <th rowspan="2">Method</th>
          <th rowspan="2">Stereo Setting</th>
          <th colspan="3" class="has-text-centered">Disparity (Â°)</th>
          <th colspan="4" class="has-text-centered">Depth (m)</th>
        </tr>
        <tr>
          <th class="has-text-centered">MAE</th>
          <th class="has-text-centered">RMSE</th>
          <th class="has-text-centered">MARE</th>
          <th class="has-text-centered">MAE</th>
          <th class="has-text-centered">RMSE</th>
          <th class="has-text-centered">MARE</th>
          <th class="has-text-centered">LRCE</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>
            <a href="https://arxiv.org/abs/1803.08669" target="_blank">PSMNet</a>
          </td>
          <td>conventional</td>
          <td class="has-text-centered">0.286</td>
          <td class="has-text-centered">0.496</td>
          <td class="has-text-centered">0.248</td>
          <td class="has-text-centered">2.509</td>
          <td class="has-text-centered">5.673</td>
          <td class="has-text-centered">0.176</td>
          <td class="has-text-centered">1.809</td>
        </tr>
        <tr>
          <td>
            <a href="https://arxiv.org/abs/1911.04460" target="_blank">360SD-Net</a>
          </td>
          <td>omnidirectional</td>
          <td class="has-text-centered">0.224</td>
          <td class="has-text-centered">0.419</td>
          <td class="has-text-centered">0.191</td>
          <td class="has-text-centered">2.122</td>
          <td class="has-text-centered">5.077</td>
          <td class="has-text-centered">0.152</td>
          <td class="has-text-centered">0.904</td>
        </tr>
        <tr>
          <td>
            <a href="https://arxiv.org/abs/2303.06615" target="_blank">IGEV-Stereo</a>
          </td>
          <td>conventional</td>
          <td class="has-text-centered">0.225</td>
          <td class="has-text-centered">0.423</td>
          <td class="has-text-centered">0.172</td>
          <td class="has-text-centered">1.860</td>
          <td class="has-text-centered">4.474</td>
          <td class="has-text-centered">0.146</td>
          <td class="has-text-centered">1.203</td>
        </tr>
        <tr>
          <td>
            <a href="https://arxiv.org/abs/2411.18335" target="_blank">360-IGEV-Stereo</a>
          </td>
          <td>omnidirectional</td>
          <td class="has-text-centered">0.188</td>
          <td class="has-text-centered">0.404</td>
          <td class="has-text-centered">0.146</td>
          <td class="has-text-centered">1.720</td>
          <td class="has-text-centered">4.297</td>
          <td class="has-text-centered">0.130</td>
          <td class="has-text-centered"><b>0.388</b></td>
        </tr>
        <tr>
          <td>
            <a href="https://arxiv.org/abs/2503.23502" target="_blank">DFI-OmniStereo</a>
          </td>
          <td>omnidirectional</td>
          <td class="has-text-centered"><b>0.158</b></td>
          <td class="has-text-centered"><b>0.338</b></td>
          <td class="has-text-centered"><b>0.120</b></td>
          <td class="has-text-centered"><b>1.463</b></td>
          <td class="has-text-centered"><b>3.767</b></td>
          <td class="has-text-centered"><b>0.108</b></td>
          <td class="has-text-centered">0.397</td>
        </tr>
      </tbody>
    </table>
    <p>
      The dataset is also an ideal testbed for assessing the robustness of depth estimation methods to diverse lighting conditions and depth ranges
      by training and evaluating models on different subsets of the dataset (e.g., indoor vs. outdoor scenes).
    </p>
      <figure>
        <img src="static/images/cross_scene_generalization.png" alt="Cross-Scene Generalization Performance">
        <figcaption>Cross-Scene Generalization Performance</figcaption>
      </figure>
    </div>
  </div>
</section>

<section class="section" id="dataset-structure">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">Download</h2>
    <p>
      Use the link below to access the dataset on HuggingFace Hub.
    </p>
    <div class="has-text-centered">
      <a href="https://huggingface.co/datasets/chcorbi/helvipad" target="_blank" class="button is-primary is-rounded is-large">
        Download Dataset
      </a>
    </div>
    <div style="margin-top: 30px;">
      <p>
        <strong>âš ï¸ Important Update (21/09/2025):</strong> A minor error was identified in the Helvipad depth-to-disparity conversion formula.
        The <em>corrected disparity maps</em> have now been regenerated and uploaded to the HuggingFace repo.
      </p>
      <ul>
        <li>The <em>legacy disparity maps</em> are still included, as they were used in the experiments reported in the Helvipad and DFI-OmniStereo papers.</li>
        <li>For any new work, we recommend using the <em>corrected disparity maps</em>.</li>
      </ul>

      <p style="margin-top: 20px;">
        The dataset is organized into training, validation, and testing subsets with the following structure:
      </p>
      <pre style="background: #f5f5f5; padding: 20px; border-radius: 5px; overflow: auto;">
<code>helvipad/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ depth_maps                          # Depth maps generated from LiDAR data
â”‚   â”œâ”€â”€ depth_maps_augmented                # Augmented depth maps using depth completion
â”‚   â”œâ”€â”€ disparity_maps_corrected            # Corrected disparity maps computed from depth maps
â”‚   â”œâ”€â”€ disparity_maps                      # /!\ Legacy version of disparity maps computed from depth maps
â”‚   â”œâ”€â”€ disparity_maps_augmented_corrected  # Corrected augmented disparity maps using depth completion
â”‚   â”œâ”€â”€ disparity_maps_augmented            # /!\ Augmented disparity maps using depth completion
â”‚   â”œâ”€â”€ images_top                          # Top-camera RGB images
â”‚   â”œâ”€â”€ images_bottom                       # Bottom-camera RGB images
â”‚   â”œâ”€â”€ LiDAR_pcd                           # Original LiDAR point cloud data
â”œâ”€â”€ val/
â”‚   â”œâ”€â”€ depth_maps                          # Depth maps generated from LiDAR data
â”‚   â”œâ”€â”€ depth_maps_augmented                # Augmented depth maps using depth completion
â”‚   â”œâ”€â”€ disparity_maps_corrected            # Corrected disparity maps computed from depth maps
â”‚   â”œâ”€â”€ disparity_maps                      # /!\ Legacy version of disparity maps computed from depth maps
â”‚   â”œâ”€â”€ disparity_maps_augmented_corrected  # Corrected augmented disparity maps using depth completion
â”‚   â”œâ”€â”€ disparity_maps_augmented            # /!\ Augmented disparity maps using depth completion
â”‚   â”œâ”€â”€ images_top                          # Top-camera RGB images
â”‚   â”œâ”€â”€ images_bottom                       # Bottom-camera RGB images
â”‚   â”œâ”€â”€ LiDAR_pcd                           # Original LiDAR point cloud data
â”œâ”€â”€ test/
â”‚   â”œâ”€â”€ depth_maps                          # Depth maps generated from LiDAR data
â”‚   â”œâ”€â”€ depth_maps_augmented                # Augmented depth maps using depth completion
â”‚   â”œâ”€â”€ disparity_maps_corrected            # Corrected disparity maps computed from depth maps
â”‚   â”œâ”€â”€ disparity_maps                      # /!\ Legacy version of disparity maps computed from depth maps
â”‚   â”œâ”€â”€ disparity_maps_augmented_corrected  # Corrected augmented disparity maps using depth completion
â”‚   â”œâ”€â”€ disparity_maps_augmented            # /!\ Augmented disparity maps using depth completion
â”‚   â”œâ”€â”€ images_top                          # Top-camera RGB images
â”‚   â”œâ”€â”€ images_bottom                       # Bottom-camera RGB images
â”‚   â”œâ”€â”€ LiDAR_pcd                           # Original LiDAR point cloud data
</code></pre>
    </div>
  </div>
</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <p>
      If you use the <span style="font-variant: small-caps;">Helvipad</span> dataset in your research, please cite it using the following BibTeX entry:
    </p>
    <pre><code>@inproceedings{zayene2025helvipad,
  author    = {Zayene, Mehdi and Endres, Jannik and Havolli, Albias and CorbiÃ¨re, Charles and Cherkaoui, Salim and Ben Ahmed Kontouli, Alexandre and Alahi, Alexandre},
  title     = {Helvipad: A Real-World Dataset for Omnidirectional Stereo Depth Estimation},
  booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2025}
}</code></pre>
  </div>
</section>


<footer class="footer">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This page was built using the
            <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>
            which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
